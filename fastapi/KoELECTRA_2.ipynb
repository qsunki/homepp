{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba95dc82-b1f9-405f-97ff-f0c96bfd0623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in English dataset: 2000\n",
      "Number of entries in Korean dataset: 2000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 경로\n",
    "file_path_en = './learn_data/expanded_security_navigation_data_en.json'\n",
    "file_path_kr = './learn_data/expanded_security_navigation_data_kr.json'\n",
    "\n",
    "# JSON 파일 열기 및 데이터 로드\n",
    "with open(file_path_en, 'r', encoding='utf-8') as f_en, open(file_path_kr, 'r', encoding='utf-8') as f_kr:\n",
    "    data_en = json.load(f_en)\n",
    "    data_kr = json.load(f_kr)\n",
    "\n",
    "# 데이터 개수 확인\n",
    "num_entries_en = len(data_en)\n",
    "num_entries_kr = len(data_kr)\n",
    "\n",
    "# 결과 출력\n",
    "print(f'Number of entries in English dataset: {num_entries_en}')\n",
    "print(f'Number of entries in Korean dataset: {num_entries_kr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad372c66-891a-430a-8297-7c973f95d58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in English dataset: 2000\n",
      "Number of entries in Korean dataset: 2000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 경로\n",
    "file_path_en = './learn_data/informal_security_navigation_data_en.json'\n",
    "file_path_kr = './learn_data/informal_security_navigation_data_kr.json'\n",
    "\n",
    "# JSON 파일 열기 및 데이터 로드\n",
    "with open(file_path_en, 'r', encoding='utf-8') as f_en, open(file_path_kr, 'r', encoding='utf-8') as f_kr:\n",
    "    data_en = json.load(f_en)\n",
    "    data_kr = json.load(f_kr)\n",
    "\n",
    "# 데이터 개수 확인\n",
    "num_entries_en = len(data_en)\n",
    "num_entries_kr = len(data_kr)\n",
    "\n",
    "# 결과 출력\n",
    "print(f'Number of entries in English dataset: {num_entries_en}')\n",
    "print(f'Number of entries in Korean dataset: {num_entries_kr}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65503814-eafd-45eb-8919-35ef5ef2a3a1",
   "metadata": {},
   "source": [
    "### 1. 환경 설정 및 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f033bd3-8609-4057-bd89-c1f4353e7cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.43.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.20.0)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nltk in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dongwook\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Collecting absl-py\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\dongwook\\appdata\\roaming\\python\\python39\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dongwook\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dongwook\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dongwook\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Using legacy 'setup.py install' for rouge-score, since package 'wheel' is not installed.\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "  Running setup.py install for rouge-score: started\n",
      "  Running setup.py install for rouge-score: finished with status 'done'\n",
      "Successfully installed absl-py-2.1.0 rouge-score-0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Dongwook\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 개수: 10018\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets rouge-score nltk\n",
    "\n",
    "import json\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification, Trainer, TrainingArguments, pipeline\n",
    "\n",
    "# JSON 파일 경로\n",
    "file_paths = [\n",
    "    './learn_data/informal_security_navigation_data_en.json',\n",
    "    './learn_data/informal_security_navigation_data_kr.json',\n",
    "    './learn_data/expanded_security_navigation_data_en.json',\n",
    "    './learn_data/expanded_security_navigation_data_kr.json',\n",
    "    './learn_data/navigation_data_en.json',\n",
    "    './learn_data/navigation_data_kr.json',\n",
    "    './learn_data/security_data_en.json',\n",
    "    './learn_data/security_data_kr.json'\n",
    "]\n",
    "\n",
    "# JSON 파일 열기 및 데이터 로드\n",
    "data = []\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data.extend(json.load(f))\n",
    "\n",
    "print(f\"총 데이터 개수: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01895bf5-cf21-4eee-bb99-70c23ca562ae",
   "metadata": {},
   "source": [
    "### 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b34912a-5a87-4e02-880e-710a57b08cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a73c0615d694cca9f1597516fbab290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/61.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dongwook\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dongwook\\.cache\\huggingface\\hub\\models--monologg--koelectra-base-v3-discriminator. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76cb1016b0942bd8153cedfcc0fbbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1470db1e8f7d4bcba3e2666fd4ba950d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/467 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebec676afd843b6a34af083fecdef8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea291c1016942078d27be82aed9abce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 9016\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'answer', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1002\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer 및 모델 로드\n",
    "tokenizer = ElectraTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "\n",
    "# 데이터셋으로 변환\n",
    "def convert_to_dataset(data):\n",
    "    questions = [item['question'] for item in data]\n",
    "    answers = [item['answer'] for item in data]\n",
    "    labels = [0 if q == a else 1 for q, a in zip(questions, answers)]  # 임의의 레이블 생성\n",
    "    dataset_dict = {'question': questions, 'answer': answers, 'labels': labels}\n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "dataset = convert_to_dataset(data)\n",
    "\n",
    "# DatasetDict 생성 (훈련, 검증 데이터셋 나누기)\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'validation': split_dataset['test']\n",
    "})\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['question'], examples['answer'], truncation=True, padding='max_length')\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47afcf60-0018-4342-8e86-9126b7077673",
   "metadata": {},
   "source": [
    "### 3. 학습 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb6043b-08fe-4837-bcc2-f676e3a87103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d847cb78b9841a08ac0c0e115dded62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/452M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraForSequenceClassification, Trainer, TrainingArguments, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.progress_bar = None\n",
    "\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        super().create_optimizer_and_scheduler(num_training_steps)\n",
    "        self.progress_bar = tqdm(total=num_training_steps, desc=\"Training Progress\")\n",
    "\n",
    "    def training_step(self, model, inputs):\n",
    "        loss = super().training_step(model, inputs)\n",
    "        self.progress_bar.update(1)\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        steps_per_second = self.state.global_step / elapsed_time if elapsed_time > 0 else float('inf')\n",
    "        remaining_time = (self.state.max_steps - self.state.global_step) / steps_per_second if steps_per_second > 0 else float('inf')\n",
    "        self.progress_bar.set_postfix({\n",
    "            \"elapsed_time\": f\"{elapsed_time // 3600:.0f}h {elapsed_time % 3600 // 60:.0f}m {elapsed_time % 60:.0f}s\",\n",
    "            \"remaining_time\": f\"{remaining_time // 3600:.0f}h {remaining_time % 3600 // 60:.0f}m {remaining_time % 60:.0f}s\" if remaining_time != float('inf') else \"Calculating...\",\n",
    "            \"loss\": loss.item()\n",
    "        })\n",
    "        return loss\n",
    "\n",
    "    def _save(self, output_dir: str, state_dict=None):\n",
    "        for param in self.model.parameters():\n",
    "            if not param.is_contiguous():\n",
    "                param.data = param.data.contiguous()\n",
    "        self.model.save_pretrained(output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors)\n",
    "        if self.tokenizer is not None:\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    def train(self, *args, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        super().train(*args, **kwargs)\n",
    "        self.progress_bar.close()\n",
    "\n",
    "# 모델 로드\n",
    "model = ElectraForSequenceClassification.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./koelectra2',             # before: './koelectra_results', after: './koelectra2'\n",
    "    eval_strategy=\"epoch\",                 # before: \"evaluation_strategy\", after: \"eval_strategy\"\n",
    "    save_strategy=\"epoch\",                 # before: \"epoch\", after: \"epoch\" (unchanged)\n",
    "    learning_rate=3e-5,                    # before: 2e-5, after: 3e-5 (학습률 증가)\n",
    "    per_device_train_batch_size=32,        # before: 16, after: 32 (배치 크기 증가)\n",
    "    per_device_eval_batch_size=32,         # before: 16, after: 32 (배치 크기 증가)\n",
    "    num_train_epochs=3,                    # before: 1, after: 3 (에포크 수 증가)\n",
    "    weight_decay=0.01,                     # before: 0.01, after: 0.01 (unchanged)\n",
    "    logging_dir='./logs',                  # before: './logs', after: './logs' (unchanged)\n",
    "    logging_steps=200,                     # before: 500, after: 200 (로그 기록 빈도 증가)\n",
    "    save_total_limit=3,                    # before: 2, after: 3 (저장 제한 수 증가)\n",
    "    fp16=True                              # 추가: 혼합 정밀도 훈련 활성화\n",
    ")\n",
    "\n",
    "# 학습률 스케줄러 추가\n",
    "total_steps = len(tokenized_datasets['train']) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_steps * 0.1,  # 초기 10% 워밍업 단계\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# CustomTrainer 설정\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    optimizers=(optimizer, scheduler)  # 최적화기와 스케줄러 추가\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8449a-69e9-48f6-955a-7056c42e7aea",
   "metadata": {},
   "source": [
    "### 4. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316ddd69-5b1c-4183-a56f-1ca8dd9da153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/846 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad63e3f40584167a826154006092a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  24%|██▎       | 200/846 [6:59:21<22:41:28, 126.45s/it, elapsed_time=6h 59m 19s, remaining_time=22h 43m 19s, loss=0.0017] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1126, 'grad_norm': 0.03061777912080288, 'learning_rate': 2.542506919731119e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  33%|███▎      | 282/846 [9:52:08<18:09:50, 115.94s/it, elapsed_time=9h 52m 9s, remaining_time=19h 50m 37s, loss=0.00118]  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56f6725cdf94ed0999f06ae3b06a0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  33%|███▎      | 282/846 [10:08:00<18:09:50, 115.94s/it, elapsed_time=9h 52m 9s, remaining_time=19h 50m 37s, loss=0.00118]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0005383248790167272, 'eval_runtime': 949.7346, 'eval_samples_per_second': 1.055, 'eval_steps_per_second': 0.034, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  47%|████▋     | 400/846 [14:20:27<16:05:09, 129.84s/it, elapsed_time=14h 20m 24s, remaining_time=16h 3m 55s, loss=0.000554] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.001, 'grad_norm': 0.01148890145123005, 'learning_rate': 1.7516805061289046e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  67%|██████▋   | 564/846 [20:13:07<8:59:48, 114.85s/it, elapsed_time=20h 13m 7s, remaining_time=10h 9m 48s, loss=0.000375]   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011a24edf41e4526b4236a710463cf82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  67%|██████▋   | 564/846 [20:28:52<8:59:48, 114.85s/it, elapsed_time=20h 13m 7s, remaining_time=10h 9m 48s, loss=0.000375]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00018665909010451287, 'eval_runtime': 941.2955, 'eval_samples_per_second': 1.064, 'eval_steps_per_second': 0.034, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  71%|███████   | 600/846 [21:45:40<8:50:33, 129.41s/it, elapsed_time=21h 45m 37s, remaining_time=8h 58m 23s, loss=0.000316] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0004, 'grad_norm': 0.0071010468527674675, 'learning_rate': 9.608540925266903e-06, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  95%|█████████▍| 800/846 [28:52:06<1:37:36, 127.31s/it, elapsed_time=28h 52m 3s, remaining_time=1h 41m 53s, loss=0.000283] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0003, 'grad_norm': 0.0063699884340167046, 'learning_rate': 1.7002767892447608e-06, 'epoch': 2.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 846/846 [30:29:37<00:00, 116.01s/it, elapsed_time=30h 29m 38s, remaining_time=0h 2m 10s, loss=0.000267]   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da76f4c17c274af684dfd513b8fc4df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 846/846 [30:45:20<00:00, 116.01s/it, elapsed_time=30h 29m 38s, remaining_time=0h 2m 10s, loss=0.000267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00014077186642680317, 'eval_runtime': 937.9464, 'eval_samples_per_second': 1.068, 'eval_steps_per_second': 0.034, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 846/846 [30:45:21<00:00, 130.88s/it, elapsed_time=30h 29m 38s, remaining_time=0h 2m 10s, loss=0.000267]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 110721.1528, 'train_samples_per_second': 0.244, 'train_steps_per_second': 0.008, 'train_loss': 0.027017994616488898, 'epoch': 3.0}\n",
      "Training completed in 30 hours, 45 minutes and 21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    # 학습 시작 시간 기록\n",
    "    trainer.start_time = time.time()\n",
    "\n",
    "    # 모델 학습\n",
    "    trainer.train()\n",
    "\n",
    "    # 학습 완료 시간 기록\n",
    "    end_time = time.time()\n",
    "\n",
    "    # 학습 시간 계산\n",
    "    training_time = end_time - trainer.start_time\n",
    "    hours, rem = divmod(training_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f'Training completed in {int(hours)} hours, {int(minutes)} minutes and {int(seconds)} seconds')\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23029af0-18ec-492b-9165-c1b3a89ba2cf",
   "metadata": {},
   "source": [
    "### 5. 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eabe3d9b-1b40-4dc6-953d-547017acbb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./koelectra2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef598108-a997-4cbd-a62e-503c64547848",
   "metadata": {},
   "source": [
    "### 6. 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b1cae98-e48b-4433-bedc-f50db468c290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686d229fa3cb495ab7e45e2f469c10f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.00014077186642680317, 'eval_runtime': 933.503, 'eval_samples_per_second': 1.073, 'eval_steps_per_second': 0.034, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(\"Evaluation Results:\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85655598-643e-4597-8a34-bf553abcc259",
   "metadata": {},
   "source": [
    "### 7. 모델 정보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdfa1c93-c8f3-4743-bbda-70a0e7dade8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Model\": \"KoELECTRA\",\n",
      "    \"Description\": \"A lightweight and efficient model optimized for Korean natural language processing.\",\n",
      "    \"Base Model\": \"ELECTRA\",\n",
      "    \"Publisher\": \"KakaoBrain\",\n",
      "    \"Advantages\": [\n",
      "        \"Efficient and fast inference\",\n",
      "        \"Good performance with limited resources\",\n",
      "        \"Optimized for Korean text classification and sentiment analysis\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 모델 정보와 특징\n",
    "model_info = {\n",
    "    \"Model\": \"KoELECTRA\",\n",
    "    \"Description\": \"A lightweight and efficient model optimized for Korean natural language processing.\",\n",
    "    \"Base Model\": \"ELECTRA\",\n",
    "    \"Publisher\": \"KakaoBrain\",\n",
    "    \"Advantages\": [\n",
    "        \"Efficient and fast inference\",\n",
    "        \"Good performance with limited resources\",\n",
    "        \"Optimized for Korean text classification and sentiment analysis\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(json.dumps(model_info, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e25a775-1599-4982-8f7b-d061a3927d72",
   "metadata": {},
   "source": [
    "### 8. 최적화 - 동적 양자화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d79a234d-db83-4ff2-a66a-b266df79b1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model quantized and saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ElectraForSequenceClassification\n",
    "\n",
    "# 모델 로드\n",
    "model_fp32 = ElectraForSequenceClassification.from_pretrained(\"./koelectra2\")\n",
    "\n",
    "# 동적 양자화 적용\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model_fp32, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# 최적화된 모델 저장\n",
    "torch.save(model_int8.state_dict(), \"./koelectra2_quantized/pytorch_model.bin\")\n",
    "print(\"Model quantized and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa622f-fc97-43fa-b09c-1923839fcd63",
   "metadata": {},
   "source": [
    "1. 동적 양자화 (Dynamic Quantization)\n",
    "- **특징**: 런타임 시 활성화 값을 양자화하며, 모델 가중치는 정밀도를 유지.\n",
    "- **장점**: \n",
    "  - 빠르고 간단하게 적용 가능\n",
    "  - CPU 기반 추론 속도 향상\n",
    "  - 메모리 사용량 감소\n",
    "- **단점**: \n",
    "  - 정적 양자화나 QAT보다 정확도가 다소 떨어질 수 있음\n",
    "- **적합한 상황**: \n",
    "  - 모델이 이미 훈련된 상태에서 재훈련 없이 최적화를 원할 때\n",
    "  - 웹 및 모바일 환경에서 CPU 기반 추론 속도를 높이고자 할 때\n",
    "\n",
    "2. 정적 양자화 (Static Quantization)\n",
    "- **특징**: 모델 가중치와 활성화 값을 양자화하며, 양자화하기 전에 일부 데이터로 모델을 '준비'.\n",
    "- **장점**: \n",
    "  - 더 높은 정확도를 제공\n",
    "  - 런타임 성능이 우수\n",
    "  - 메모리 사용량과 추론 속도 모두 최적화\n",
    "- **단점**: \n",
    "  - 일부 데이터를 사용하여 모델을 '준비'하는 추가 단계 필요\n",
    "- **적합한 상황**: \n",
    "  - 추론 시 정확도와 성능 모두를 개선하고자 할 때\n",
    "  - 데이터 샘플을 사용하여 모델을 준비할 수 있을 때\n",
    "\n",
    "3. 지연 양자화 (Quantization-Aware Training, QAT)\n",
    "- **특징**: 양자화된 모델을 훈련하여 정확도를 유지함.\n",
    "- **장점**: \n",
    "  - 양자화로 인한 성능 손실 최소화\n",
    "  - 높은 정확도 유지\n",
    "- **단점**: \n",
    "  - 전체 훈련 파이프라인을 다시 실행해야 함\n",
    "  - 훈련 시간이 길어질 수 있음\n",
    "- **적합한 상황**: \n",
    "  - 양자화로 인한 성능 저하를 최소화하고 정확도를 유지하면서 최적화를 하고자 할 때\n",
    "  - 훈련에 필요한 리소스와 시간이 충분할 때\n",
    "\n",
    "4. 프루닝 (Pruning)\n",
    "- **특징**: 모델의 파라미터 수를 줄여 경량화함.\n",
    "- **장점**: \n",
    "  - 모델의 크기와 메모리 사용량 감소\n",
    "  - 특정 기준에 따라 중요도가 낮은 파라미터를 제거하여 모델을 최적화\n",
    "- **단점**: \n",
    "  - 프루닝 후 재훈련 필요\n",
    "  - 잘못된 프루닝으로 인해 성능 저하 가능\n",
    "- **적합한 상황**: \n",
    "  - 모델의 크기를 줄이고 메모리 사용량을 줄이고자 할 때\n",
    "  - 프루닝 후 재훈련을 통해 모델 성능을 유지할 수 있을 때\n",
    "\n",
    "5. 혼합 정밀도 훈련 (Mixed Precision Training)\n",
    "- **특징**: 모델의 일부 파라미터를 반 정밀도로 훈련하여 메모리 사용량을 줄이고 훈련 속도를 높임.\n",
    "- **장점**: \n",
    "  - 훈련 속도 향상\n",
    "  - 메모리 사용량 감소\n",
    "- **단점**: \n",
    "  - 모든 하드웨어에서 지원되지 않을 수 있음\n",
    "  - 모델의 안정성에 영향을 미칠 수 있음\n",
    "- **적합한 상황**: \n",
    "  - 훈련 시간이 긴 대형 모델의 훈련 속도를 높이고자 할 때\n",
    "  - GPU를 활용하여 훈련하는 경우\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6158c-d502-4b18-87b8-6137bdec0a38",
   "metadata": {},
   "source": [
    "### 9. 시뮬레이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa85815a-cdae-4a13-96d6-5e40948eaa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dongwook\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_utils.py:383: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n",
      "Some weights of the model checkpoint at ./koelectra2_quantized were not used when initializing ElectraForSequenceClassification: ['classifier.dense._packed_params._packed_params', 'classifier.dense._packed_params.dtype', 'classifier.dense.scale', 'classifier.dense.zero_point', 'classifier.out_proj._packed_params._packed_params', 'classifier.out_proj._packed_params.dtype', 'classifier.out_proj.scale', 'classifier.out_proj.zero_point', 'electra.encoder.layer.0.attention.output.dense._packed_params._packed_params', 'electra.encoder.layer.0.attention.output.dense._packed_params.dtype', 'electra.encoder.layer.0.attention.output.dense.scale', 'electra.encoder.layer.0.attention.output.dense.zero_point', 'electra.encoder.layer.0.attention.self.key._packed_params._packed_params', 'electra.encoder.layer.0.attention.self.key._packed_params.dtype', 'electra.encoder.layer.0.attention.self.key.scale', 'electra.encoder.layer.0.attention.self.key.zero_point', 'electra.encoder.layer.0.attention.self.query._packed_params._packed_params', 'electra.encoder.layer.0.attention.self.query._packed_params.dtype', 'electra.encoder.layer.0.attention.self.query.scale', 'electra.encoder.layer.0.attention.self.query.zero_point', 'electra.encoder.layer.0.attention.self.value._packed_params._packed_params', 'electra.encoder.layer.0.attention.self.value._packed_params.dtype', 'electra.encoder.layer.0.attention.self.value.scale', 'electra.encoder.layer.0.attention.self.value.zero_point', 'electra.encoder.layer.0.intermediate.dense._packed_params._packed_params', 'electra.encoder.layer.0.intermediate.dense._packed_params.dtype', 'electra.encoder.layer.0.intermediate.dense.scale', 'electra.encoder.layer.0.intermediate.dense.zero_point', 'electra.encoder.layer.0.output.dense._packed_params._packed_params', 'electra.encoder.layer.0.output.dense._packed_params.dtype', 'electra.encoder.layer.0.output.dense.scale', 'electra.encoder.layer.0.output.dense.zero_point', 'electra.encoder.layer.1.attention.output.dense._packed_params._packed_params', 'electra.encoder.layer.1.attention.output.dense._packed_params.dtype', 'electra.encoder.layer.1.attention.output.dense.scale', 'electra.encoder.layer.1.attention.output.dense.zero_point', 'electra.encoder.layer.1.attention.self.key._packed_params._packed_params', 'electra.encoder.layer.1.attention.self.key._packed_params.dtype', 'electra.encoder.layer.1.attention.self.key.scale', 'electra.encoder.layer.1.attention.self.key.zero_point', 'electra.encoder.layer.1.attention.self.query._packed_params._packed_params', 'electra.encoder.layer.1.attention.self.query._packed_params.dtype', 'electra.encoder.layer.1.attention.self.query.scale', 'electra.encoder.layer.1.attention.self.query.zero_point', 'electra.encoder.layer.1.attention.self.value._packed_params._packed_params', 'electra.encoder.layer.1.attention.self.value._packed_params.dtype', 'electra.encoder.layer.1.attention.self.value.scale', 'electra.encoder.layer.1.attention.self.value.zero_point', 'electra.encoder.layer.1.intermediate.dense._packed_params._packed_params', 'electra.encoder.layer.1.intermediate.dense._packed_params.dtype', 'electra.encoder.layer.1.intermediate.dense.scale', 'electra.encoder.layer.1.intermediate.dense.zero_point', 'electra.encoder.layer.1.output.dense._packed_params._packed_params', 'electra.encoder.layer.1.output.dense._packed_params.dtype', 'electra.encoder.layer.1.output.dense.scale', 'electra.encoder.layer.1.output.dense.zero_point', 'electra.encoder.layer.10.attention.output.dense._packed_params._packed_params', 'electra.encoder.layer.10.attention.output.dense._packed_params.dtype', 'electra.encoder.layer.10.attention.output.dense.scale', 'electra.encoder.layer.10.attention.output.dense.zero_point', 'electra.encoder.layer.10.attention.self.key._packed_params._packed_params', 'electra.encoder.layer.10.attention.self.key._packed_params.dtype', 'electra.encoder.layer.10.attention.self.key.scale', 'electra.encoder.layer.10.attention.self.key.zero_point', 'electra.encoder.layer.10.attention.self.query._packed_params._packed_params', 'electra.encoder.layer.10.attention.self.query._packed_params.dtype', 'electra.encoder.layer.10.attention.self.query.scale', 'electra.encoder.layer.10.attention.self.query.zero_point', 'electra.encoder.layer.10.attention.self.value._packed_params._packed_params', 'electra.encoder.layer.10.attention.self.value._packed_params.dtype', 'electra.encoder.layer.10.attention.self.value.scale', 'electra.encoder.layer.10.attention.self.value.zero_point', 'electra.encoder.layer.10.intermediate.dense._packed_params._packed_params', 'electra.encoder.layer.10.intermediate.dense._packed_params.dtype', 'electra.encoder.layer.10.intermediate.dense.scale', 'electra.encoder.layer.10.intermediate.dense.zero_point', 'electra.encoder.layer.10.output.dense._packed_params._packed_params', 'electra.encoder.layer.10.output.dense._packed_params.dtype', 'electra.encoder.layer.10.output.dense.scale', 'electra.encoder.layer.10.output.dense.zero_point', 'electra.encoder.layer.11.attention.output.dense._packed_params._packed_params', 'electra.encoder.layer.11.attention.output.dense._packed_params.dtype', 'electra.encoder.layer.11.attention.output.dense.scale', 'electra.encoder.layer.11.attention.output.dense.zero_point', 'electra.encoder.layer.11.attention.self.key._packed_params._packed_params', 'electra.encoder.layer.11.attention.self.key._packed_params.dtype', 'electra.encoder.layer.11.attention.self.key.scale', 'electra.encoder.layer.11.attention.self.key.zero_point', 'electra.encoder.layer.11.attention.self.query._packed_params._packed_params', 'electra.encoder.layer.11.attention.self.query._packed_params.dtype', 'electra.encoder.layer.11.attention.self.query.scale', 'electra.encoder.layer.11.attention.self.query.zero_point', 'electra.encoder.layer.11.attention.self.value._packed_params._packed_params', 'electra.encoder.layer.11.attention.self.value._packed_params.dtype', 'electra.encoder.layer.11.attention.self.value.scale', 'electra.encoder.layer.11.attention.self.value.zero_point', 'electra.encoder.layer.11.intermediate.dense._packed_params._packed_params', 'electra.encoder.layer.11.intermediate.dense._packed_params.dtype', 'electra.encoder.layer.11.intermediate.dense.scale', 'electra.encoder.layer.11.intermediate.dense.zero_point', 'electra.encoder.layer.11.output.dense._packed_params._packed_params', 'electra.encoder.layer.11.output.dense._packed_params.dtype', 'electra.encoder.layer.11.output.dense.scale', 'electra.encoder.layer.11.output.dense.zero_point', 'electra.encoder.layer.2.attention.output.dense._packed_params._packed_params', 'electra.encoder.layer.2.attention.output.dense._packed_params.dtype', 'electra.encoder.layer.2.attention.output.dense.scale', 'electra.encoder.layer.2.attention.output.dense.zero_point', 'electra.encoder.layer.2.attention.self.key._packed_params._packed_params', 'electra.encoder.layer.2.attention.self.key._packed_params.dtype', 'electra.encoder.layer.2.attention.self.key.scale', 'electra.encoder.layer.2.attention.self.key.zero_point', 'electra.encoder.layer.2.attention.self.query._packed_params._packed_params', 'electra.encoder.layer.2.attention.self.query._packed_params.dtype', 'electra.encoder.layer.2.attention.self.query.scale', 'electra.encoder.layer.2.attention.self.query.zero_point', 'electra.encoder.layer.2.attention.self.value._packed_params._packed_params', 'electra.encoder.layer.2.attention.self.value._packed_params.dtype', 'electra.encoder.layer.2.attention.self.value.scale', 'electra.encoder.layer.2.attention.self.value.zero_point', 'electra.encoder.layer.2.intermediate.dense._packed_params._packed_params', 'electra.encoder.layer.2.intermediate.dense._packed_params.dtype', 'electra.encoder.layer.2.intermediate.dense.scale', 'electra.encoder.layer.2.intermediate.dense.zero_point', 'electra.encoder.layer.2.output.dense._packed_params._packed_params', 'electra.encoder.layer.2.output.dense._packed_params.dtype', 'electra.encoder.layer.2.output.dense.scale', 'electra.encoder.layer.2.output.dense.zero_point', 'electra.encoder.layer.3.attention.output.dense._packed_params._packed_params', 'electra.encoder.layer.3.attention.output.dense._packed_params.dtype', 'electra.encoder.layer.3.attention.output.dense.scale', 'electra.encoder.layer.3.attention.output.dense.zero_point', 'electra.encoder.layer.3.attention.self.key._packed_params._packed_params', 'electra.encoder.layer.3.attention.self.key._packed_params.dtype', 'electra.encoder.layer.3.attention.self.key.scale', 'electra.encoder.layer.3.attention.self.key.zero_point', 'electra.encoder.layer.3.attention.self.query._packed_params._packed_params', 'electra.encoder.layer.3.attention.self.query._packed_params.dtype', 'electra.encoder.layer.3.attention.self.query.scale', 'electra.encoder.layer.3.attention.self.query.zero_point', 'electra.encoder.layer.3.attention.self.value._packed_params._packed_params', 'electra.encoder.layer.3.attention.self.value._packed_params.dtype', 'electra.encoder.layer.3.attention.self.value.scale', 'electra.encoder.layer.3.attention.self.value.zero_point', 'electra.encoder.layer.3.intermediate.dense._packed_params._packed_params', 'electra.encoder.layer.3.intermediate.dense._packed_params.dtype', 'electra.encoder.layer.3.intermediate.dense.scale', 'electra.encoder.layer.3.intermediate.dense.zero_point', 'electra.encoder.layer.3.output.dense._packed_params._packed_params', 'electra.encoder.layer.3.output.dense._packed_params.dtype', 'electra.encoder.layer.3.output.dense.scale', 'electra.encoder.layer.3.output.dense.zero_point', 'electra.encoder.layer.4.attention.output.dense._packed_params._packed_params', 'electra.encoder.layer.4.attention.output.dense._packed_params.dtype', 'electra.encoder.layer.4.attention.output.dense.scale', 'electra.encoder.layer.4.attention.output.dense.zero_point', 'electra.encoder.layer.4.attention.self.key._packed_params._packed_params', 'electra.encoder.layer.4.attention.self.key._packed_params.dtype', 'electra.encoder.layer.4.attention.self.key.scale', 'electra.encoder.layer.4.attention.self.key.zero_point', 'electra.encoder.layer.4.attention.self.query._packed_params._packed_params', 'electra.encoder.layer.4.attention.self.query._packed_params.dtype', 'electra.encoder.layer.4.attention.self.query.scale', 'electra.encoder.layer.4.attention.self.query.zero_point', 'electra.encoder.layer.4.attention.self.value._packed_params._packed_params', 'electra.encoder.layer.4.attention.self.value._packed_params.dtype', 'electra.encoder.layer.4.attention.self.value.scale', 'electra.encoder.layer.4.attention.self.value.zero_point', 'electra.encoder.layer.4.intermediate.dense._packed_params._packed_params', 'electra.encoder.layer.4.intermediate.dense._packed_params.dtype', 'electra.encoder.layer.4.intermediate.dense.scale', 'electra.encoder.layer.4.intermediate.dense.zero_point', 'electra.encoder.layer.4.output.dense._packed_params._packed_params', 'electra.encoder.layer.4.output.dense._packed_params.dtype', 'electra.encoder.layer.4.output.dense.scale', 'electra.encoder.layer.4.output.dense.zero_point', 'electra.encoder.layer.5.attention.output.dense._packed_params._packed_params', 'electra.encoder.layer.5.attention.output.dense._packed_params.dtype', 'electra.encoder.layer.5.attention.output.dense.scale', 'electra.encoder.layer.5.attention.output.dense.zero_point', 'electra.encoder.layer.5.attention.self.key._packed_params._packed_params', 'electra.encoder.layer.5.attention.self.key._packed_params.dtype', 'electra.encoder.layer.5.attention.self.key.scale', 'electra.encoder.layer.5.attention.self.key.zero_point', 'electra.encoder.layer.5.attention.self.query._packed_params._packed_params', 'electra.encoder.layer.5.attention.self.query._packed_params.dtype', 'electra.encoder.layer.5.attention.self.query.scale', 'electra.encoder.layer.5.attention.self.query.zero_point', 'electra.encoder.layer.5.attention.self.value._packed_params._packed_params', 'electra.encoder.layer.5.attention.self.value._packed_params.dtype', 'electra.encoder.layer.5.attention.self.value.scale', 'electra.encoder.layer.5.attention.self.value.zero_point', 'electra.encoder.layer.5.intermediate.dense._packed_params._packed_params', 'electra.encoder.layer.5.intermediate.dense._packed_params.dtype', 'electra.encoder.layer.5.intermediate.dense.scale', 'electra.encoder.layer.5.intermediate.dense.zero_point', 'electra.encoder.layer.5.output.dense._packed_params._packed_params', 'electra.encoder.layer.5.output.dense._packed_params.dtype', 'electra.encoder.layer.5.output.dense.scale', 'electra.encoder.layer.5.output.dense.zero_point', 'electra.encoder.layer.6.attention.output.dense._packed_params._packed_params', 'electra.encoder.layer.6.attention.output.dense._packed_params.dtype', 'electra.encoder.layer.6.attention.output.dense.scale', 'electra.encoder.layer.6.attention.output.dense.zero_point', 'electra.encoder.layer.6.attention.self.key._packed_params._packed_params', 'electra.encoder.layer.6.attention.self.key._packed_params.dtype', 'electra.encoder.layer.6.attention.self.key.scale', 'electra.encoder.layer.6.attention.self.key.zero_point', 'electra.encoder.layer.6.attention.self.query._packed_params._packed_params', 'electra.encoder.layer.6.attention.self.query._packed_params.dtype', 'electra.encoder.layer.6.attention.self.query.scale', 'electra.encoder.layer.6.attention.self.query.zero_point', 'electra.encoder.layer.6.attention.self.value._packed_params._packed_params', 'electra.encoder.layer.6.attention.self.value._packed_params.dtype', 'electra.encoder.layer.6.attention.self.value.scale', 'electra.encoder.layer.6.attention.self.value.zero_point', 'electra.encoder.layer.6.intermediate.dense._packed_params._packed_params', 'electra.encoder.layer.6.intermediate.dense._packed_params.dtype', 'electra.encoder.layer.6.intermediate.dense.scale', 'electra.encoder.layer.6.intermediate.dense.zero_point', 'electra.encoder.layer.6.output.dense._packed_params._packed_params', 'electra.encoder.layer.6.output.dense._packed_params.dtype', 'electra.encoder.layer.6.output.dense.scale', 'electra.encoder.layer.6.output.dense.zero_point', 'electra.encoder.layer.7.attention.output.dense._packed_params._packed_params', 'electra.encoder.layer.7.attention.output.dense._packed_params.dtype', 'electra.encoder.layer.7.attention.output.dense.scale', 'electra.encoder.layer.7.attention.output.dense.zero_point', 'electra.encoder.layer.7.attention.self.key._packed_params._packed_params', 'electra.encoder.layer.7.attention.self.key._packed_params.dtype', 'electra.encoder.layer.7.attention.self.key.scale', 'electra.encoder.layer.7.attention.self.key.zero_point', 'electra.encoder.layer.7.attention.self.query._packed_params._packed_params', 'electra.encoder.layer.7.attention.self.query._packed_params.dtype', 'electra.encoder.layer.7.attention.self.query.scale', 'electra.encoder.layer.7.attention.self.query.zero_point', 'electra.encoder.layer.7.attention.self.value._packed_params._packed_params', 'electra.encoder.layer.7.attention.self.value._packed_params.dtype', 'electra.encoder.layer.7.attention.self.value.scale', 'electra.encoder.layer.7.attention.self.value.zero_point', 'electra.encoder.layer.7.intermediate.dense._packed_params._packed_params', 'electra.encoder.layer.7.intermediate.dense._packed_params.dtype', 'electra.encoder.layer.7.intermediate.dense.scale', 'electra.encoder.layer.7.intermediate.dense.zero_point', 'electra.encoder.layer.7.output.dense._packed_params._packed_params', 'electra.encoder.layer.7.output.dense._packed_params.dtype', 'electra.encoder.layer.7.output.dense.scale', 'electra.encoder.layer.7.output.dense.zero_point', 'electra.encoder.layer.8.attention.output.dense._packed_params._packed_params', 'electra.encoder.layer.8.attention.output.dense._packed_params.dtype', 'electra.encoder.layer.8.attention.output.dense.scale', 'electra.encoder.layer.8.attention.output.dense.zero_point', 'electra.encoder.layer.8.attention.self.key._packed_params._packed_params', 'electra.encoder.layer.8.attention.self.key._packed_params.dtype', 'electra.encoder.layer.8.attention.self.key.scale', 'electra.encoder.layer.8.attention.self.key.zero_point', 'electra.encoder.layer.8.attention.self.query._packed_params._packed_params', 'electra.encoder.layer.8.attention.self.query._packed_params.dtype', 'electra.encoder.layer.8.attention.self.query.scale', 'electra.encoder.layer.8.attention.self.query.zero_point', 'electra.encoder.layer.8.attention.self.value._packed_params._packed_params', 'electra.encoder.layer.8.attention.self.value._packed_params.dtype', 'electra.encoder.layer.8.attention.self.value.scale', 'electra.encoder.layer.8.attention.self.value.zero_point', 'electra.encoder.layer.8.intermediate.dense._packed_params._packed_params', 'electra.encoder.layer.8.intermediate.dense._packed_params.dtype', 'electra.encoder.layer.8.intermediate.dense.scale', 'electra.encoder.layer.8.intermediate.dense.zero_point', 'electra.encoder.layer.8.output.dense._packed_params._packed_params', 'electra.encoder.layer.8.output.dense._packed_params.dtype', 'electra.encoder.layer.8.output.dense.scale', 'electra.encoder.layer.8.output.dense.zero_point', 'electra.encoder.layer.9.attention.output.dense._packed_params._packed_params', 'electra.encoder.layer.9.attention.output.dense._packed_params.dtype', 'electra.encoder.layer.9.attention.output.dense.scale', 'electra.encoder.layer.9.attention.output.dense.zero_point', 'electra.encoder.layer.9.attention.self.key._packed_params._packed_params', 'electra.encoder.layer.9.attention.self.key._packed_params.dtype', 'electra.encoder.layer.9.attention.self.key.scale', 'electra.encoder.layer.9.attention.self.key.zero_point', 'electra.encoder.layer.9.attention.self.query._packed_params._packed_params', 'electra.encoder.layer.9.attention.self.query._packed_params.dtype', 'electra.encoder.layer.9.attention.self.query.scale', 'electra.encoder.layer.9.attention.self.query.zero_point', 'electra.encoder.layer.9.attention.self.value._packed_params._packed_params', 'electra.encoder.layer.9.attention.self.value._packed_params.dtype', 'electra.encoder.layer.9.attention.self.value.scale', 'electra.encoder.layer.9.attention.self.value.zero_point', 'electra.encoder.layer.9.intermediate.dense._packed_params._packed_params', 'electra.encoder.layer.9.intermediate.dense._packed_params.dtype', 'electra.encoder.layer.9.intermediate.dense.scale', 'electra.encoder.layer.9.intermediate.dense.zero_point', 'electra.encoder.layer.9.output.dense._packed_params._packed_params', 'electra.encoder.layer.9.output.dense._packed_params.dtype', 'electra.encoder.layer.9.output.dense.scale', 'electra.encoder.layer.9.output.dense.zero_point']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at ./koelectra2_quantized and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'electra.encoder.layer.0.attention.output.dense.bias', 'electra.encoder.layer.0.attention.output.dense.weight', 'electra.encoder.layer.0.attention.self.key.bias', 'electra.encoder.layer.0.attention.self.key.weight', 'electra.encoder.layer.0.attention.self.query.bias', 'electra.encoder.layer.0.attention.self.query.weight', 'electra.encoder.layer.0.attention.self.value.bias', 'electra.encoder.layer.0.attention.self.value.weight', 'electra.encoder.layer.0.intermediate.dense.bias', 'electra.encoder.layer.0.intermediate.dense.weight', 'electra.encoder.layer.0.output.dense.bias', 'electra.encoder.layer.0.output.dense.weight', 'electra.encoder.layer.1.attention.output.dense.bias', 'electra.encoder.layer.1.attention.output.dense.weight', 'electra.encoder.layer.1.attention.self.key.bias', 'electra.encoder.layer.1.attention.self.key.weight', 'electra.encoder.layer.1.attention.self.query.bias', 'electra.encoder.layer.1.attention.self.query.weight', 'electra.encoder.layer.1.attention.self.value.bias', 'electra.encoder.layer.1.attention.self.value.weight', 'electra.encoder.layer.1.intermediate.dense.bias', 'electra.encoder.layer.1.intermediate.dense.weight', 'electra.encoder.layer.1.output.dense.bias', 'electra.encoder.layer.1.output.dense.weight', 'electra.encoder.layer.10.attention.output.dense.bias', 'electra.encoder.layer.10.attention.output.dense.weight', 'electra.encoder.layer.10.attention.self.key.bias', 'electra.encoder.layer.10.attention.self.key.weight', 'electra.encoder.layer.10.attention.self.query.bias', 'electra.encoder.layer.10.attention.self.query.weight', 'electra.encoder.layer.10.attention.self.value.bias', 'electra.encoder.layer.10.attention.self.value.weight', 'electra.encoder.layer.10.intermediate.dense.bias', 'electra.encoder.layer.10.intermediate.dense.weight', 'electra.encoder.layer.10.output.dense.bias', 'electra.encoder.layer.10.output.dense.weight', 'electra.encoder.layer.11.attention.output.dense.bias', 'electra.encoder.layer.11.attention.output.dense.weight', 'electra.encoder.layer.11.attention.self.key.bias', 'electra.encoder.layer.11.attention.self.key.weight', 'electra.encoder.layer.11.attention.self.query.bias', 'electra.encoder.layer.11.attention.self.query.weight', 'electra.encoder.layer.11.attention.self.value.bias', 'electra.encoder.layer.11.attention.self.value.weight', 'electra.encoder.layer.11.intermediate.dense.bias', 'electra.encoder.layer.11.intermediate.dense.weight', 'electra.encoder.layer.11.output.dense.bias', 'electra.encoder.layer.11.output.dense.weight', 'electra.encoder.layer.2.attention.output.dense.bias', 'electra.encoder.layer.2.attention.output.dense.weight', 'electra.encoder.layer.2.attention.self.key.bias', 'electra.encoder.layer.2.attention.self.key.weight', 'electra.encoder.layer.2.attention.self.query.bias', 'electra.encoder.layer.2.attention.self.query.weight', 'electra.encoder.layer.2.attention.self.value.bias', 'electra.encoder.layer.2.attention.self.value.weight', 'electra.encoder.layer.2.intermediate.dense.bias', 'electra.encoder.layer.2.intermediate.dense.weight', 'electra.encoder.layer.2.output.dense.bias', 'electra.encoder.layer.2.output.dense.weight', 'electra.encoder.layer.3.attention.output.dense.bias', 'electra.encoder.layer.3.attention.output.dense.weight', 'electra.encoder.layer.3.attention.self.key.bias', 'electra.encoder.layer.3.attention.self.key.weight', 'electra.encoder.layer.3.attention.self.query.bias', 'electra.encoder.layer.3.attention.self.query.weight', 'electra.encoder.layer.3.attention.self.value.bias', 'electra.encoder.layer.3.attention.self.value.weight', 'electra.encoder.layer.3.intermediate.dense.bias', 'electra.encoder.layer.3.intermediate.dense.weight', 'electra.encoder.layer.3.output.dense.bias', 'electra.encoder.layer.3.output.dense.weight', 'electra.encoder.layer.4.attention.output.dense.bias', 'electra.encoder.layer.4.attention.output.dense.weight', 'electra.encoder.layer.4.attention.self.key.bias', 'electra.encoder.layer.4.attention.self.key.weight', 'electra.encoder.layer.4.attention.self.query.bias', 'electra.encoder.layer.4.attention.self.query.weight', 'electra.encoder.layer.4.attention.self.value.bias', 'electra.encoder.layer.4.attention.self.value.weight', 'electra.encoder.layer.4.intermediate.dense.bias', 'electra.encoder.layer.4.intermediate.dense.weight', 'electra.encoder.layer.4.output.dense.bias', 'electra.encoder.layer.4.output.dense.weight', 'electra.encoder.layer.5.attention.output.dense.bias', 'electra.encoder.layer.5.attention.output.dense.weight', 'electra.encoder.layer.5.attention.self.key.bias', 'electra.encoder.layer.5.attention.self.key.weight', 'electra.encoder.layer.5.attention.self.query.bias', 'electra.encoder.layer.5.attention.self.query.weight', 'electra.encoder.layer.5.attention.self.value.bias', 'electra.encoder.layer.5.attention.self.value.weight', 'electra.encoder.layer.5.intermediate.dense.bias', 'electra.encoder.layer.5.intermediate.dense.weight', 'electra.encoder.layer.5.output.dense.bias', 'electra.encoder.layer.5.output.dense.weight', 'electra.encoder.layer.6.attention.output.dense.bias', 'electra.encoder.layer.6.attention.output.dense.weight', 'electra.encoder.layer.6.attention.self.key.bias', 'electra.encoder.layer.6.attention.self.key.weight', 'electra.encoder.layer.6.attention.self.query.bias', 'electra.encoder.layer.6.attention.self.query.weight', 'electra.encoder.layer.6.attention.self.value.bias', 'electra.encoder.layer.6.attention.self.value.weight', 'electra.encoder.layer.6.intermediate.dense.bias', 'electra.encoder.layer.6.intermediate.dense.weight', 'electra.encoder.layer.6.output.dense.bias', 'electra.encoder.layer.6.output.dense.weight', 'electra.encoder.layer.7.attention.output.dense.bias', 'electra.encoder.layer.7.attention.output.dense.weight', 'electra.encoder.layer.7.attention.self.key.bias', 'electra.encoder.layer.7.attention.self.key.weight', 'electra.encoder.layer.7.attention.self.query.bias', 'electra.encoder.layer.7.attention.self.query.weight', 'electra.encoder.layer.7.attention.self.value.bias', 'electra.encoder.layer.7.attention.self.value.weight', 'electra.encoder.layer.7.intermediate.dense.bias', 'electra.encoder.layer.7.intermediate.dense.weight', 'electra.encoder.layer.7.output.dense.bias', 'electra.encoder.layer.7.output.dense.weight', 'electra.encoder.layer.8.attention.output.dense.bias', 'electra.encoder.layer.8.attention.output.dense.weight', 'electra.encoder.layer.8.attention.self.key.bias', 'electra.encoder.layer.8.attention.self.key.weight', 'electra.encoder.layer.8.attention.self.query.bias', 'electra.encoder.layer.8.attention.self.query.weight', 'electra.encoder.layer.8.attention.self.value.bias', 'electra.encoder.layer.8.attention.self.value.weight', 'electra.encoder.layer.8.intermediate.dense.bias', 'electra.encoder.layer.8.intermediate.dense.weight', 'electra.encoder.layer.8.output.dense.bias', 'electra.encoder.layer.8.output.dense.weight', 'electra.encoder.layer.9.attention.output.dense.bias', 'electra.encoder.layer.9.attention.output.dense.weight', 'electra.encoder.layer.9.attention.self.key.bias', 'electra.encoder.layer.9.attention.self.key.weight', 'electra.encoder.layer.9.attention.self.query.bias', 'electra.encoder.layer.9.attention.self.query.weight', 'electra.encoder.layer.9.attention.self.value.bias', 'electra.encoder.layer.9.attention.self.value.weight', 'electra.encoder.layer.9.intermediate.dense.bias', 'electra.encoder.layer.9.intermediate.dense.weight', 'electra.encoder.layer.9.output.dense.bias', 'electra.encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 이 영화 정말 재미있어요!\n",
      "Prediction: [{'label': 'LABEL_0', 'score': 0.5522649884223938}]\n",
      "\n",
      "Text: 서비스가 별로였어요.\n",
      "Prediction: [{'label': 'LABEL_0', 'score': 0.5521101951599121}]\n",
      "\n",
      "Text: 가격 대비 품질이 훌륭합니다.\n",
      "Prediction: [{'label': 'LABEL_0', 'score': 0.5509616136550903}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 정량화된 모델 로드\n",
    "quantized_model = ElectraForSequenceClassification.from_pretrained(\"./koelectra2_quantized\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "\n",
    "# 파이프라인 생성\n",
    "nlp = pipeline(\"text-classification\", model=quantized_model, tokenizer=tokenizer)\n",
    "\n",
    "# 샘플 텍스트로 시뮬레이션 수행\n",
    "sample_texts = [\n",
    "    \"이 영화 정말 재미있어요!\",\n",
    "    \"서비스가 별로였어요.\",\n",
    "    \"가격 대비 품질이 훌륭합니다.\"\n",
    "]\n",
    "\n",
    "# 예측 수행 및 결과 출력\n",
    "for text in sample_texts:\n",
    "    result = nlp(text)\n",
    "    print(f\"Text: {text}\\nPrediction: {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723a891-4c6e-447f-a05c-503f8cbd3705",
   "metadata": {},
   "source": [
    "### 10. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3aa3857-869e-4cea-9448-e3c664282c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02a5a9ba062495cb8bff6d132431bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# 평가 데이터에 대한 예측 수행\n",
    "predictions = trainer.predict(tokenized_datasets['validation'])\n",
    "y_true = predictions.label_ids\n",
    "y_pred = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# 정밀도 계산\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "\n",
    "# 재현율 계산\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# F1 점수 계산\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "# 혼동 행렬 계산\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff6b45-3133-4023-84ae-e8ed04d17122",
   "metadata": {},
   "source": [
    "1. BLEU (Bilingual Evaluation Understudy) Score:\n",
    "\n",
    "- 기계 번역 및 텍스트 생성 모델의 품질을 평가하기 위해 사용되는 지표입니다.\n",
    "- 모델이 생성한 텍스트와 참조 텍스트 간의 n-gram 일치를 측정합니다.\n",
    "\n",
    "2. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) Score:\n",
    "- 텍스트 요약 및 생성 모델의 품질을 평가하기 위해 사용됩니다.\n",
    "- ROUGE-N, ROUGE-L 등의 변형이 있으며, 주로 n-gram, LCS(Longest Common Subsequence)을 기반으로 측정합니다.\n",
    "\n",
    "3. Perplexity:\n",
    "- 언어 모델의 예측 성능을 평가하는 지표입니다.\n",
    "- 낮을수록 모델이 테스트 데이터의 분포를 더 잘 예측한다는 것을 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a4ccb-c6f7-444a-b11e-f8504cc7a7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# BLEU 및 ROUGE 메트릭 로드\n",
    "bleu = load_metric('bleu')\n",
    "rouge = load_metric('rouge')\n",
    "\n",
    "# 실제 응답과 모델 응답 예시\n",
    "references = [\n",
    "    [\"this is a test\", \"this is a trial\"],\n",
    "    [\"another test here\"]\n",
    "]\n",
    "predictions = [\n",
    "    \"this is a test\",\n",
    "    \"another test\"\n",
    "]\n",
    "\n",
    "# BLEU 점수 계산\n",
    "bleu_result = bleu.compute(predictions=predictions, references=references)\n",
    "print(f\"BLEU Score: {bleu_result['bleu']}\")\n",
    "\n",
    "# ROUGE 점수 계산\n",
    "rouge_result = rouge.compute(predictions=predictions, references=references)\n",
    "print(f\"ROUGE Score: {rouge_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17dafb6-c477-42ef-9783-0deb0b56a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = ElectraTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "model = ElectraForSequenceClassification.from_pretrained('./koelectra2')\n",
    "\n",
    "# 테스트 데이터 예시\n",
    "test_texts = [\n",
    "    \"이 영화 정말 재미있어요!\",\n",
    "    \"서비스가 별로였어요.\"\n",
    "]\n",
    "\n",
    "# 텍스트 토크나이즈\n",
    "inputs = tokenizer(test_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# 모델 예측 수행\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "\n",
    "# Perplexity 계산\n",
    "perplexity = torch.exp(loss)\n",
    "print(f\"Perplexity: {perplexity.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b756a4-a5b9-4ff4-b67e-4c1635aa2cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a0ac4-9420-44ea-89ea-68555c32d1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6dfa6-7e3c-4ab3-8e03-7640f4f91079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
